{
    "batch_size": 512,
    "token_dim": 1,
    "learning_rate": 0.0003,
    "weight_decay": 0.0001,
    "attention_dim": 4096,
    "num_attention_heads": 64,
    "embedding_dim": 128,
    "feed_forward_dim": 512,
    "num_blocks": 24,
    "grad_clip": 2.0,
    "activation_fn": "gelu",
    "dtype": "bfloat16",
    "remat": false
}
